AWS Complete Tutorial & Real-Time Projects
Step-by-step guide from beginner to advanced with hands-on projects

ðŸš€ PHASE 1: AWS FOUNDATION (Days 1-7)
Day 1: AWS Account Setup & IAM
Step 1: Create AWS Account
bash# Go to aws.amazon.com
# Click "Create an AWS Account"
# Provide email, password, account name
# Add payment method (won't be charged in free tier)
# Verify phone number
# Choose Basic support plan (free)
Step 2: Secure Your Root Account
bash# 1. Enable MFA (Multi-Factor Authentication)
#    - Go to IAM Console â†’ Root user â†’ Security credentials
#    - Assign MFA device (use Google Authenticator app)

# 2. Create IAM Admin User (NEVER use root for daily tasks)
#    - Go to IAM â†’ Users â†’ Add User
#    - Username: admin-user
#    - Access type: AWS Management Console access + Programmatic access
#    - Attach policy: AdministratorAccess
#    - Download CSV with credentials
Step 3: Install AWS CLI
bash# On Windows (using PowerShell)
msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi

# On macOS
curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
sudo installer -pkg AWSCLIV2.pkg -target /

# On Linux
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# Configure AWS CLI
aws configure
# AWS Access Key ID: [Your Access Key]
# AWS Secret Access Key: [Your Secret Key]
# Default region name: us-east-1
# Default output format: json

# Test configuration
aws sts get-caller-identity
Hands-On Practice Day 1:
bash# Create your first IAM user via CLI
aws iam create-user --user-name test-user

# List all users
aws iam list-users

# Create access key for user
aws iam create-access-key --user-name test-user

# Delete user (cleanup)
aws iam delete-user --user-name test-user

Day 2: EC2 (Elastic Compute Cloud)
Step 1: Launch Your First EC2 Instance
bash# Via AWS Console:
# 1. Go to EC2 Dashboard
# 2. Click "Launch Instance"
# 3. Choose AMI: Amazon Linux 2023
# 4. Instance type: t2.micro (free tier)
# 5. Create new key pair: my-first-keypair.pem
# 6. Security group: Allow SSH (port 22)
# 7. Launch instance

# Via AWS CLI:
aws ec2 run-instances \
    --image-id ami-0abcdef1234567890 \
    --count 1 \
    --instance-type t2.micro \
    --key-name my-first-keypair \
    --security-group-ids sg-12345678

# List your instances
aws ec2 describe-instances
Step 2: Connect to Your Instance
bash# Change key permissions (Linux/Mac)
chmod 400 my-first-keypair.pem

# Connect via SSH
ssh -i "my-first-keypair.pem" ec2-user@your-public-ip

# On Windows, use PuTTY or WSL
Step 3: Basic Server Setup
bash# Once connected to your instance:
sudo yum update -y                    # Update system
sudo yum install -y httpd            # Install Apache
sudo systemctl start httpd           # Start web server
sudo systemctl enable httpd          # Enable on boot

# Create simple webpage
echo "<h1>My First AWS Web Server!</h1>" | sudo tee /var/www/html/index.html

# Test: Visit http://your-public-ip in browser
Hands-On Practice Day 2:
bash# Create security group for web server
aws ec2 create-security-group \
    --group-name web-server-sg \
    --description "Security group for web server"

# Add rules to allow HTTP traffic
aws ec2 authorize-security-group-ingress \
    --group-name web-server-sg \
    --protocol tcp \
    --port 80 \
    --cidr 0.0.0.0/0

# Launch instance with new security group
aws ec2 run-instances \
    --image-id ami-0abcdef1234567890 \
    --count 1 \
    --instance-type t2.micro \
    --key-name my-first-keypair \
    --security-groups web-server-sg

Day 3: S3 (Simple Storage Service)
Step 1: Create S3 Bucket
bash# Via AWS Console:
# 1. Go to S3 service
# 2. Click "Create bucket"
# 3. Bucket name: my-first-bucket-uniquename123
# 4. Region: us-east-1
# 5. Keep default settings
# 6. Create bucket

# Via AWS CLI:
aws s3 mb s3://my-first-bucket-uniquename123

# List buckets
aws s3 ls
Step 2: Upload and Manage Files
bash# Create a test file
echo "Hello from AWS S3!" > test-file.txt

# Upload file to S3
aws s3 cp test-file.txt s3://my-first-bucket-uniquename123/

# List objects in bucket
aws s3 ls s3://my-first-bucket-uniquename123/

# Download file from S3
aws s3 cp s3://my-first-bucket-uniquename123/test-file.txt downloaded-file.txt

# Sync entire directory
mkdir my-website
echo "<h1>My Website</h1>" > my-website/index.html
aws s3 sync my-website/ s3://my-first-bucket-uniquename123/website/
Step 3: Static Website Hosting
bash# Enable static website hosting
aws s3 website s3://my-first-bucket-uniquename123 \
    --index-document index.html \
    --error-document error.html

# Create bucket policy for public access
cat > bucket-policy.json << EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::my-first-bucket-uniquename123/*"
        }
    ]
}
EOF

# Apply bucket policy
aws s3api put-bucket-policy \
    --bucket my-first-bucket-uniquename123 \
    --policy file://bucket-policy.json
Hands-On Practice Day 3:
bash# Upload multiple files with metadata
aws s3 cp local-file.txt s3://my-bucket/ \
    --metadata "author=john,project=demo"

# Set up lifecycle policy to delete objects after 30 days
cat > lifecycle.json << EOF
{
    "Rules": [
        {
            "Status": "Enabled",
            "Filter": {"Prefix": "temp/"},
            "Expiration": {"Days": 30}
        }
    ]
}
EOF

aws s3api put-bucket-lifecycle-configuration \
    --bucket my-first-bucket-uniquename123 \
    --lifecycle-configuration file://lifecycle.json

Day 4: VPC (Virtual Private Cloud)
Step 1: Create Custom VPC
bash# Create VPC
aws ec2 create-vpc --cidr-block 10.0.0.0/16

# Tag your VPC
aws ec2 create-tags \
    --resources vpc-12345678 \
    --tags Key=Name,Value=MyCustomVPC

# Enable DNS hostnames
aws ec2 modify-vpc-attribute \
    --vpc-id vpc-12345678 \
    --enable-dns-hostnames
Step 2: Create Subnets
bash# Create public subnet
aws ec2 create-subnet \
    --vpc-id vpc-12345678 \
    --cidr-block 10.0.1.0/24 \
    --availability-zone us-east-1a

# Create private subnet  
aws ec2 create-subnet \
    --vpc-id vpc-12345678 \
    --cidr-block 10.0.2.0/24 \
    --availability-zone us-east-1b

# Tag subnets
aws ec2 create-tags \
    --resources subnet-12345678 \
    --tags Key=Name,Value=PublicSubnet

aws ec2 create-tags \
    --resources subnet-87654321 \
    --tags Key=Name,Value=PrivateSubnet
Step 3: Internet Gateway and Routing
bash# Create Internet Gateway
aws ec2 create-internet-gateway

# Attach to VPC
aws ec2 attach-internet-gateway \
    --internet-gateway-id igw-12345678 \
    --vpc-id vpc-12345678

# Create route table for public subnet
aws ec2 create-route-table --vpc-id vpc-12345678

# Add route to internet gateway
aws ec2 create-route \
    --route-table-id rtb-12345678 \
    --destination-cidr-block 0.0.0.0/0 \
    --gateway-id igw-12345678

# Associate with public subnet
aws ec2 associate-route-table \
    --route-table-id rtb-12345678 \
    --subnet-id subnet-12345678
Hands-On Practice Day 4:
bash# Launch instance in custom VPC
aws ec2 run-instances \
    --image-id ami-0abcdef1234567890 \
    --count 1 \
    --instance-type t2.micro \
    --key-name my-keypair \
    --subnet-id subnet-12345678 \
    --associate-public-ip-address

# Create NAT Gateway for private subnet internet access
aws ec2 allocate-address --domain vpc
aws ec2 create-nat-gateway \
    --subnet-id subnet-12345678 \
    --allocation-id eipalloc-12345678

Day 5: RDS (Relational Database Service)
Step 1: Create RDS Instance
bash# Create DB subnet group
aws rds create-db-subnet-group \
    --db-subnet-group-name my-db-subnet-group \
    --db-subnet-group-description "My DB subnet group" \
    --subnet-ids subnet-12345678 subnet-87654321

# Create RDS instance
aws rds create-db-instance \
    --db-instance-identifier mydb \
    --db-instance-class db.t3.micro \
    --engine mysql \
    --master-username admin \
    --master-user-password mypassword123 \
    --allocated-storage 20 \
    --db-subnet-group-name my-db-subnet-group \
    --vpc-security-group-ids sg-12345678
Step 2: Connect to Database
bash# Install MySQL client on EC2 instance
sudo yum install -y mysql

# Connect to RDS
mysql -h mydb.abcdef123456.us-east-1.rds.amazonaws.com -u admin -p

# Create database and table
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(50), email VARCHAR(100));
INSERT INTO users VALUES (1, 'John Doe', 'john@example.com');
SELECT * FROM users;
Hands-On Practice Day 5:
bash# Create read replica
aws rds create-db-instance-read-replica \
    --db-instance-identifier mydb-replica \
    --source-db-instance-identifier mydb

# Create automated backup
aws rds modify-db-instance \
    --db-instance-identifier mydb \
    --backup-retention-period 7 \
    --apply-immediately

Day 6: Load Balancer & Auto Scaling
Step 1: Application Load Balancer
bash# Create target group
aws elbv2 create-target-group \
    --name my-target-group \
    --protocol HTTP \
    --port 80 \
    --vpc-id vpc-12345678 \
    --health-check-path /

# Create load balancer
aws elbv2 create-load-balancer \
    --name my-load-balancer \
    --subnets subnet-12345678 subnet-87654321 \
    --security-groups sg-12345678

# Create listener
aws elbv2 create-listener \
    --load-balancer-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/app/my-load-balancer/1234567890123456 \
    --protocol HTTP \
    --port 80 \
    --default-actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/my-target-group/1234567890123456
Step 2: Auto Scaling Group
bash# Create launch template
aws ec2 create-launch-template \
    --launch-template-name my-launch-template \
    --launch-template-data '{
        "ImageId": "ami-0abcdef1234567890",
        "InstanceType": "t2.micro",
        "KeyName": "my-keypair",
        "SecurityGroupIds": ["sg-12345678"],
        "UserData": "IyEvYmluL2Jhc2gKc3VkbyB5dW0gdXBkYXRlIC15CnN1ZG8geXVtIGluc3RhbGwgLXkgaHR0cGQKc3VkbyBzeXN0ZW1jdGwgc3RhcnQgaHR0cGQKc3VkbyBzeXN0ZW1jdGwgZW5hYmxlIGh0dHBkCmVjaG8gIjxoMT5BdXRvIFNjYWxlZCBJbnN0YW5jZSE8L2gxPiIgfCBzdWRvIHRlZSAvdmFyL3d3dy9odG1sL2luZGV4Lmh0bWw="
    }'

# Create Auto Scaling Group
aws autoscaling create-auto-scaling-group \
    --auto-scaling-group-name my-asg \
    --launch-template LaunchTemplateName=my-launch-template,Version=1 \
    --min-size 1 \
    --max-size 3 \
    --desired-capacity 2 \
    --target-group-arns arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/my-target-group/1234567890123456 \
    --vpc-zone-identifier "subnet-12345678,subnet-87654321"
Hands-On Practice Day 6:
bash# Create scaling policies
aws autoscaling put-scaling-policy \
    --auto-scaling-group-name my-asg \
    --policy-name scale-up-policy \
    --policy-type StepScaling \
    --adjustment-type PercentChangeInCapacity \
    --metric-aggregation-type Average \
    --step-adjustments MetricIntervalLowerBound=0,ScalingAdjustment=50

# Create CloudWatch alarm for scaling
aws cloudwatch put-metric-alarm \
    --alarm-name HighCPUUtilization \
    --alarm-description "Scale up on high CPU" \
    --metric-name CPUUtilization \
    --namespace AWS/EC2 \
    --statistic Average \
    --period 300 \
    --evaluation-periods 2 \
    --threshold 80 \
    --comparison-operator GreaterThanThreshold \
    --alarm-actions arn:aws:autoscaling:us-east-1:123456789012:scalingPolicy:policy-id:autoScalingGroupName/my-asg:policyName/scale-up-policy

Day 7: Lambda & CloudWatch
Step 1: Create Lambda Function
python# Create lambda_function.py
import json
import boto3

def lambda_handler(event, context):
    # Process the event
    print(f"Received event: {json.dumps(event)}")
    
    # Example: Get S3 object information
    if 'Records' in event:
        for record in event['Records']:
            bucket = record['s3']['bucket']['name']
            key = record['s3']['object']['key']
            print(f"File {key} uploaded to bucket {bucket}")
    
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
bash# Create deployment package
zip lambda-deployment.zip lambda_function.py

# Create Lambda function
aws lambda create-function \
    --function-name my-lambda-function \
    --runtime python3.9 \
    --role arn:aws:iam::123456789012:role/lambda-execution-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://lambda-deployment.zip

# Test Lambda function
aws lambda invoke \
    --function-name my-lambda-function \
    --payload '{"test": "data"}' \
    response.json
Step 2: CloudWatch Monitoring
bash# Create custom metric
aws cloudwatch put-metric-data \
    --namespace "MyApp/Performance" \
    --metric-data MetricName=ResponseTime,Value=245,Unit=Milliseconds,Timestamp=2023-01-01T12:00:00Z

# Create CloudWatch alarm
aws cloudwatch put-metric-alarm \
    --alarm-name "High-Response-Time" \
    --alarm-description "Alarm when response time exceeds 500ms" \
    --metric-name ResponseTime \
    --namespace MyApp/Performance \
    --statistic Average \
    --period 300 \
    --evaluation-periods 2 \
    --threshold 500 \
    --comparison-operator GreaterThanThreshold \
    --alarm-actions arn:aws:sns:us-east-1:123456789012:my-topic
Hands-On Practice Day 7:
bash# Set up S3 event trigger for Lambda
aws lambda add-permission \
    --function-name my-lambda-function \
    --principal s3.amazonaws.com \
    --action lambda:InvokeFunction \
    --statement-id s3-trigger \
    --source-arn arn:aws:s3:::my-bucket

# Configure S3 bucket notification
aws s3api put-bucket-notification-configuration \
    --bucket my-bucket \
    --notification-configuration '{
        "LambdaConfigurations": [
            {
                "Id": "ObjectCreated",
                "LambdaFunctionArn": "arn:aws:lambda:us-east-1:123456789012:function:my-lambda-function",
                "Events": ["s3:ObjectCreated:*"]
            }
        ]
    }'

ðŸ—ï¸ PHASE 2: REAL-TIME PROJECTS (Days 8-30)
PROJECT 1: 3-Tier Web Application (Days 8-12)
Project Overview:
Build a complete 3-tier application with presentation layer (web server), application layer (API), and data layer (database).
Architecture:
Internet Gateway
    â†“
Application Load Balancer (Public Subnet)
    â†“
Auto Scaling Group - Web Servers (Private Subnet)
    â†“
Application Load Balancer (Private Subnet)
    â†“
Auto Scaling Group - API Servers (Private Subnet)
    â†“
RDS MySQL (Private Subnet)
Step-by-Step Implementation:
bash# Day 8: Infrastructure Setup
# 1. Create VPC and Subnets
aws ec2 create-vpc --cidr-block 10.0.0.0/16
aws ec2 create-subnet --vpc-id vpc-xxx --cidr-block 10.0.1.0/24  # Public
aws ec2 create-subnet --vpc-id vpc-xxx --cidr-block 10.0.2.0/24  # Private Web
aws ec2 create-subnet --vpc-id vpc-xxx --cidr-block 10.0.3.0/24  # Private App
aws ec2 create-subnet --vpc-id vpc-xxx --cidr-block 10.0.4.0/24  # Private DB

# 2. Create Internet Gateway and NAT Gateway
aws ec2 create-internet-gateway
aws ec2 attach-internet-gateway --vpc-id vpc-xxx --internet-gateway-id igw-xxx
aws ec2 create-nat-gateway --subnet-id subnet-public --allocation-id eipalloc-xxx

# 3. Create Security Groups
aws ec2 create-security-group --group-name web-tier-sg --description "Web tier security group" --vpc-id vpc-xxx
aws ec2 create-security-group --group-name app-tier-sg --description "App tier security group" --vpc-id vpc-xxx
aws ec2 create-security-group --group-name db-tier-sg --description "DB tier security group" --vpc-id vpc-xxx
Web Tier Setup (Day 9):
bash# Create user data script for web servers
cat > web-server-userdata.sh << 'EOF'
#!/bin/bash
yum update -y
yum install -y httpd php php-mysql
systemctl start httpd
systemctl enable httpd

# Create PHP application
cat > /var/www/html/index.php << 'PHPEOF'
<?php
$servername = "RDS_ENDPOINT";
$username = "admin";
$password = "password123";
$dbname = "webapp";

try {
    $pdo = new PDO("mysql:host=$servername;dbname=$dbname", $username, $password);
    $pdo->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
    
    $stmt = $pdo->prepare("SELECT COUNT(*) as count FROM visits");
    $stmt->execute();
    $result = $stmt->fetch(PDO::FETCH_ASSOC);
    
    echo "<h1>3-Tier Web Application</h1>";
    echo "<p>Total visits: " . $result['count'] . "</p>";
    echo "<p>Server: " . gethostname() . "</p>";
    
    // Increment visit counter
    $stmt = $pdo->prepare("INSERT INTO visits (timestamp) VALUES (NOW())");
    $stmt->execute();
    
} catch(PDOException $e) {
    echo "Connection failed: " . $e->getMessage();
}
?>
PHPEOF
EOF

# Create launch template for web tier
aws ec2 create-launch-template \
    --launch-template-name web-tier-template \
    --launch-template-data file://web-tier-template.json
Database Tier Setup (Day 10):
bash# Create RDS subnet group
aws rds create-db-subnet-group \
    --db-subnet-group-name webapp-db-subnet-group \
    --db-subnet-group-description "Web app database subnet group" \
    --subnet-ids subnet-db1 subnet-db2

# Create RDS instance
aws rds create-db-instance \
    --db-instance-identifier webapp-database \
    --db-instance-class db.t3.micro \
    --engine mysql \
    --master-username admin \
    --master-user-password password123 \
    --allocated-storage 20 \
    --db-name webapp \
    --db-subnet-group-name webapp-db-subnet-group \
    --vpc-security-group-ids sg-db-tier

# Initialize database
mysql -h webapp-database.xxx.us-east-1.rds.amazonaws.com -u admin -p << 'EOF'
USE webapp;
CREATE TABLE visits (
    id INT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME
);
EOF
Load Balancer & Auto Scaling (Day 11):
bash# Create Application Load Balancer
aws elbv2 create-load-balancer \
    --name webapp-alb \
    --subnets subnet-public1 subnet-public2 \
    --security-groups sg-web-alb

# Create target group
aws elbv2 create-target-group \
    --name webapp-targets \
    --protocol HTTP \
    --port 80 \
    --vpc-id vpc-xxx \
    --health-check-path /index.php

# Create Auto Scaling Group
aws autoscaling create-auto-scaling-group \
    --auto-scaling-group-name webapp-asg \
    --launch-template LaunchTemplateName=web-tier-template,Version=1 \
    --min-size 2 \
    --max-size 6 \
    --desired-capacity 2 \
    --target-group-arns arn:aws:elasticloadbalancing:region:account:targetgroup/webapp-targets/xxx \
    --vpc-zone-identifier "subnet-web1,subnet-web2"
Monitoring & Alerts (Day 12):
bash# Create CloudWatch dashboard
aws cloudwatch put-dashboard \
    --dashboard-name "WebApp-Dashboard" \
    --dashboard-body file://dashboard.json

# Create auto-scaling policies
aws autoscaling put-scaling-policy \
    --auto-scaling-group-name webapp-asg \
    --policy-name webapp-scale-up \
    --policy-type TargetTrackingScaling \
    --target-tracking-configuration '{
        "TargetValue": 70.0,
        "PredefinedMetricSpecification": {
            "PredefinedMetricType": "ASGAverageCPUUtilization"
        }
    }'

PROJECT 2: Serverless Data Processing Pipeline (Days 13-17)
Project Overview:
Build a serverless data processing pipeline that automatically processes files uploaded to S3.
Architecture:
S3 Bucket (Raw Data) 
    â†“ (Event Trigger)
Lambda Function (Data Processor)
    â†“
S3 Bucket (Processed Data)
    â†“ (Event Trigger)
Lambda Function (Analytics)
    â†“
DynamoDB (Results Storage)
    â†“
CloudWatch (Monitoring)
Step-by-Step Implementation:
python# Day 13: Data Processing Lambda
# data_processor.py
import json
import boto3
import csv
import io
from datetime import datetime

s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

def lambda_handler(event, context):
    # Get bucket and object info from S3 event
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    try:
        # Download CSV file from S3
        response = s3.get_object(Bucket=bucket, Key=key)
        csv_content = response['Body'].read().decode('utf-8')
        
        # Process CSV data
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        processed_data = []
        
        for row in csv_reader:
            # Example processing: calculate total sales
            processed_row = {
                'product': row['product'],
                'sales': float(row['sales']),
                'profit': float(row['sales']) * 0.2,  # 20% profit margin
                'timestamp': datetime.now().isoformat()
            }
            processed_data.append(processed_row)
        
        # Save processed data to S3
        processed_key = f"processed/{key.replace('.csv', '_processed.json')}"
        s3.put_object(
            Bucket='my-processed-data-bucket',
            Key=processed_key,
            Body=json.dumps(processed_data),
            ContentType='application/json'
        )
        
        # Save summary to DynamoDB
        table = dynamodb.Table('ProcessingSummary')
        table.put_item(
            Item={
                'file_name': key,
                'processed_at': datetime.now().isoformat(),
                'record_count': len(processed_data),
                'total_sales': sum([row['sales'] for row in processed_data]),
                'total_profit': sum([row['profit'] for row in processed_data])
            }
        )
        
        return {
            'statusCode': 200,
            'body': json.dumps(f'Successfully processed {len(processed_data)} records')
        }
        
    except Exception as e:
        print(f"Error processing file {key}: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {str(e)}')
        }
bash# Day 14: Infrastructure Setup
# Create S3 buckets
aws s3 mb s3://my-raw-data-bucket-unique123
aws s3 mb s3://my-processed-data-bucket-unique123

# Create DynamoDB table
aws dynamodb create-table \
    --table-name ProcessingSummary \
    --attribute-definitions \
        AttributeName=file_name,AttributeType=S \
    --key-schema \
        AttributeName=file_name,KeyType=HASH \
    --billing-mode PAY_PER_REQUEST

# Create IAM role for Lambda
aws iam create-role \
    --role-name DataProcessingLambdaRole \
    --assume-role-policy-document file://lambda-trust-policy.json

aws iam attach-role-policy \
    --role-name DataProcessingLambdaRole \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

# Create custom policy for S3 and DynamoDB access
aws iam create-policy \
    --policy-name DataProcessingPolicy \
    --policy-document file://data-processing-policy.json

aws iam attach-role-policy \
    --role-name DataProcessingLambdaRole \
    --policy-arn arn:aws:iam::account-id:policy/DataProcessingPolicy
Analytics Lambda (Day 15):
python# analytics_processor.py
import json
import boto3
from decimal import Decimal

cloudwatch = boto3.client('cloudwatch')
dynamodb = boto3.resource('dynamodb')

def lambda_handler(event, context):
    table = dynamodb.Table('ProcessingSummary')
    
    try:
        # Scan all processing records
        response = table.scan()
        items = response['Items']
        
        # Calculate analytics
        total_files = len(items)
        total_records = sum([int(item['record_count']) for item in items])
        total_sales = sum([float(item['total_sales']) for item in items])
